{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPl8M5uvD7jqlTzsFDXpdsL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aicreativeexplorer/YT-Automation/blob/main/YT_Video_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Po1pvckPCdRP",
        "outputId": "610e88ad-253f-4ced-888c-15a60c38e818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.6/811.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.5/728.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m788.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q torch==2.2.0+cu118 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q diffusers transformers accelerate safetensors\n",
        "!pip install -q flask flask_cors\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q flask-cors\n",
        "import os\n",
        "import gc\n",
        "import math\n",
        "import time\n",
        "import uuid\n",
        "import threading\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import imageio\n",
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "from diffusers import StableVideoDiffusionPipeline, StableDiffusionXLPipeline\n",
        "from flask import Flask, request, jsonify, send_file\n",
        "from flask_cors import CORS\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# CONFIG\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "# Where to store generated videos\n",
        "GEN_DIR = Path(\"/content/generated_videos\")\n",
        "GEN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Hugging Face token (set it in Colab or hardcode if you want)\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\").strip()\n",
        "\n",
        "# Model IDs\n",
        "SVD_XL_ID   = \"stabilityai/stable-video-diffusion-img2vid-xt\"   # primary\n",
        "SVD_BASE_ID = \"stabilityai/stable-video-diffusion-img2vid\"      # fallback\n",
        "SDXL_ID     = \"stabilityai/stable-diffusion-xl-base-1.0\"        # text→image\n",
        "\n",
        "# Video settings\n",
        "TARGET_SECONDS = 10          # 10-second videos\n",
        "FPS            = 6           # 6 fps => ~60 frames for 10s (SVD will clamp)\n",
        "HEIGHT         = 576         # SVD default-friendly res\n",
        "WIDTH          = 1024\n",
        "\n",
        "# Global model cache\n",
        "GLOBAL_SVD_PIPE   = None\n",
        "GLOBAL_SVD_MODEL  = None     # which SVD is active (XL or BASE)\n",
        "GLOBAL_SDXL_PIPE  = None\n",
        "\n",
        "# Job registry\n",
        "JOBS = {}  # job_id -> { status, output_path, error?, prompt, seed_url, model_used }\n",
        "JOBS_LOCK = threading.Lock()\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"DEVICE =\", DEVICE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDhAnRCoC10q",
        "outputId": "127514f3-75b8-4edd-aa47-719ba8c70215"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE = cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sdxl():\n",
        "    \"\"\"Load SDXL once for text→image (realistic/photoreal hybrid).\"\"\"\n",
        "    global GLOBAL_SDXL_PIPE\n",
        "\n",
        "    if GLOBAL_SDXL_PIPE is not None:\n",
        "        print(\"[SDXL] Reusing cached SDXL pipeline\")\n",
        "        return GLOBAL_SDXL_PIPE\n",
        "\n",
        "    auth = HF_TOKEN if HF_TOKEN else True\n",
        "    print(\"[SDXL] Loading SDXL base model on\", DEVICE)\n",
        "\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        SDXL_ID,\n",
        "        torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
        "        use_auth_token=auth,\n",
        "    )\n",
        "\n",
        "    if DEVICE == \"cuda\":\n",
        "        pipe.to(\"cuda\")\n",
        "        pipe.enable_attention_slicing(\"max\")\n",
        "    else:\n",
        "        pipe.to(\"cpu\")\n",
        "\n",
        "    GLOBAL_SDXL_PIPE = pipe\n",
        "    print(\"[SDXL] Loaded\")\n",
        "    return pipe\n",
        "\n",
        "\n",
        "def load_svd():\n",
        "    \"\"\"\n",
        "    Load SVD-XL first, fallback to SVD-base if OOM or failure.\n",
        "    Returns (pipe, model_name)\n",
        "    \"\"\"\n",
        "    global GLOBAL_SVD_PIPE, GLOBAL_SVD_MODEL\n",
        "\n",
        "    if GLOBAL_SVD_PIPE is not None:\n",
        "        print(f\"[SVD] Reusing cached SVD: {GLOBAL_SVD_MODEL}\")\n",
        "        return GLOBAL_SVD_PIPE, GLOBAL_SVD_MODEL\n",
        "\n",
        "    auth = HF_TOKEN if HF_TOKEN else True\n",
        "\n",
        "    def _load(model_id):\n",
        "        print(f\"[SVD] Loading {model_id} on {DEVICE}\")\n",
        "        pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
        "            use_auth_token=auth,\n",
        "        )\n",
        "        if DEVICE == \"cuda\":\n",
        "            pipe.to(\"cuda\")\n",
        "            pipe.enable_model_cpu_offload()\n",
        "        else:\n",
        "            pipe.to(\"cpu\")\n",
        "        return pipe\n",
        "\n",
        "    # Try XL first\n",
        "    try:\n",
        "        pipe = _load(SVD_XL_ID)\n",
        "        GLOBAL_SVD_PIPE = pipe\n",
        "        GLOBAL_SVD_MODEL = \"SVD_XL\"\n",
        "        print(\"[SVD] Loaded SVD-XL\")\n",
        "        return pipe, GLOBAL_SVD_MODEL\n",
        "    except Exception as e:\n",
        "        print(\"[SVD] Failed loading SVD-XL:\", repr(e))\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Fallback to base\n",
        "    try:\n",
        "        pipe = _load(SVD_BASE_ID)\n",
        "        GLOBAL_SVD_PIPE = pipe\n",
        "        GLOBAL_SVD_MODEL = \"SVD_BASE\"\n",
        "        print(\"[SVD] Loaded SVD-BASE as fallback\")\n",
        "        return pipe, GLOBAL_SVD_MODEL\n",
        "    except Exception as e:\n",
        "        print(\"[SVD] Failed loading SVD-BASE too:\", repr(e))\n",
        "        raise RuntimeError(\"Could not load any SVD model\")\n"
      ],
      "metadata": {
        "id": "re-_QIWjDgKO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_image(url: str, dest: Path) -> Path:\n",
        "    \"\"\"Download image from URL to dest.\"\"\"\n",
        "    resp = requests.get(url, timeout=30)\n",
        "    resp.raise_for_status()\n",
        "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(dest, \"wb\") as f:\n",
        "        f.write(resp.content)\n",
        "    return dest\n",
        "\n",
        "\n",
        "def frames_to_video(frames, out_path: Path, fps: int = FPS):\n",
        "    \"\"\"Convert list of PIL images to mp4 with imageio-ffmpeg.\"\"\"\n",
        "    out_path = Path(out_path)\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    writer = imageio.get_writer(out_path, fps=fps, codec=\"libx264\")\n",
        "    for frame in frames:\n",
        "        writer.append_data(imageio.asarray(frame))\n",
        "    writer.close()\n",
        "    return out_path\n",
        "\n",
        "\n",
        "def gpu_keepalive_loop():\n",
        "    \"\"\"Tiny CUDA ops to keep GPU 'awake' and reduce deallocation.\"\"\"\n",
        "    if DEVICE != \"cuda\":\n",
        "        print(\"[KEEPALIVE] Not on GPU, skipping\")\n",
        "        return\n",
        "    print(\"[KEEPALIVE] GPU keepalive thread started\")\n",
        "    while True:\n",
        "        try:\n",
        "            x = torch.randn((128, 128), device=\"cuda\")\n",
        "            y = x * 1.0000001\n",
        "            del x, y\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "        except Exception as e:\n",
        "            print(\"[KEEPALIVE] Error:\", repr(e))\n",
        "        time.sleep(120)  # every 2 min\n",
        "\n",
        "\n",
        "# Start keepalive in background\n",
        "keepalive_thread = threading.Thread(target=gpu_keepalive_loop, daemon=True)\n",
        "keepalive_thread.start()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGKTjTWtDiFm",
        "outputId": "6e331b68-251e-4344-9afc-8c0d42d5a5a3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[KEEPALIVE] Not on GPU, skipping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image_from_text(prompt: str) -> Image.Image:\n",
        "    \"\"\"Use SDXL to create a high-quality still frame.\"\"\"\n",
        "    pipe = load_sdxl()\n",
        "\n",
        "    # Slightly photoreal / cinematic hybrid\n",
        "    enhanced_prompt = (\n",
        "        f\"{prompt}, ultra high quality, cinematic lighting, photorealistic, \"\n",
        "        \"sharp details, 4k, masterpiece\"\n",
        "    )\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        img = pipe(\n",
        "            enhanced_prompt,\n",
        "            height=HEIGHT,\n",
        "            width=WIDTH,\n",
        "            num_inference_steps=30,\n",
        "        ).images[0]\n",
        "    return img\n",
        "\n",
        "\n",
        "def generate_video_svd(job_id: str, prompt: str, seed_image: Image.Image, duration: int = TARGET_SECONDS) -> Path:\n",
        "    \"\"\"\n",
        "    Use SVD (XL or base) to create a video from one seed image.\n",
        "    \"\"\"\n",
        "    pipe, model_name = load_svd()\n",
        "\n",
        "    # SVD expects PIL upscaled to correct res\n",
        "    img = seed_image.convert(\"RGB\")\n",
        "    img = img.resize((WIDTH, HEIGHT), Image.LANCZOS)\n",
        "\n",
        "    num_frames = min(25, duration * FPS)  # clamp frames\n",
        "    cfg_scale  = 2.5\n",
        "\n",
        "    print(f\"[SVD] Running {model_name} for job {job_id}: {num_frames} frames, {duration}s\")\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        result = pipe(\n",
        "            img,\n",
        "            num_frames=num_frames,\n",
        "            decode_chunk_size=8,\n",
        "            motion_bucket_id=127,  # mid-motion\n",
        "            fps=FPS,\n",
        "            noise_aug_strength=0.02,\n",
        "            guidance_scale=cfg_scale,\n",
        "        )\n",
        "\n",
        "    frames = result.frames[0]  # (num_frames, H, W, C)\n",
        "\n",
        "    # Convert to PILs and video\n",
        "    pil_frames = [Image.fromarray(frame) for frame in frames]\n",
        "    out_path = GEN_DIR / f\"{job_id}.mp4\"\n",
        "    frames_to_video(pil_frames, out_path, fps=FPS)\n",
        "\n",
        "    return out_path, model_name\n"
      ],
      "metadata": {
        "id": "Rb33WVEzDm7h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "\n",
        "@app.route(\"/health\", methods=[\"GET\"])\n",
        "def health():\n",
        "    gpu = torch.cuda.is_available()\n",
        "    return jsonify({\"ok\": True, \"gpu\": gpu})\n",
        "\n",
        "\n",
        "@app.route(\"/api/generate\", methods=[\"POST\"])\n",
        "def api_generate():\n",
        "    \"\"\"\n",
        "    Synchronous GPU job:\n",
        "    - If seed_url provided → download + image→video\n",
        "    - Else → text→image→video using SDXL + SVD\n",
        "    Contract:\n",
        "    - Request JSON: { id, prompt, duration, mode, seed_url? }\n",
        "    - Response JSON: { ok: true/false, jobId, error? }\n",
        "    \"\"\"\n",
        "    global JOBS\n",
        "\n",
        "    data = request.get_json(silent=True) or request.form.to_dict() or {}\n",
        "\n",
        "    job_id = data.get(\"id\") or f\"job-{uuid.uuid4().hex[:8]}\"\n",
        "    prompt = data.get(\"prompt\", \"A cinematic landscape\")\n",
        "    duration = int(data.get(\"duration\", TARGET_SECONDS))\n",
        "    duration = max(2, min(duration, 12))  # clamp to 2–12 sec\n",
        "    seed_url = data.get(\"seed_url\")\n",
        "\n",
        "    with JOBS_LOCK:\n",
        "        JOBS[job_id] = {\n",
        "            \"status\": \"running\",\n",
        "            \"prompt\": prompt,\n",
        "            \"seed_url\": seed_url,\n",
        "            \"output_path\": None,\n",
        "            \"error\": None,\n",
        "            \"model_used\": None,\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        # 1) Get seed image: image URL or SDXL\n",
        "        if seed_url:\n",
        "            img_path = GEN_DIR / f\"{job_id}_seed.jpg\"\n",
        "            print(f\"[API] Downloading seed image for job {job_id}\")\n",
        "            download_image(seed_url, img_path)\n",
        "            seed_img = Image.open(img_path).convert(\"RGB\")\n",
        "        else:\n",
        "            print(f\"[API] Generating seed image from text for job {job_id}\")\n",
        "            seed_img = generate_image_from_text(prompt)\n",
        "\n",
        "        # 2) SVD: image→video\n",
        "        video_path, model_name = generate_video_svd(job_id, prompt, seed_img, duration)\n",
        "\n",
        "        with JOBS_LOCK:\n",
        "            JOBS[job_id][\"status\"] = \"done\"\n",
        "            JOBS[job_id][\"output_path\"] = str(video_path)\n",
        "            JOBS[job_id][\"model_used\"] = model_name\n",
        "\n",
        "        print(f\"[API] Job {job_id} DONE -> {video_path}\")\n",
        "        return jsonify({\"ok\": True, \"jobId\": job_id})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[API] Job {job_id} ERROR:\", repr(e))\n",
        "        with JOBS_LOCK:\n",
        "            JOBS[job_id][\"status\"] = \"error\"\n",
        "            JOBS[job_id][\"error\"] = repr(e)\n",
        "        return jsonify({\"ok\": False, \"jobId\": job_id, \"error\": repr(e)}), 500\n",
        "\n",
        "\n",
        "@app.route(\"/api/output/<job_id>\", methods=[\"GET\"])\n",
        "def api_output(job_id):\n",
        "    \"\"\"\n",
        "    Serve the generated MP4 for a given job.\n",
        "    \"\"\"\n",
        "    with JOBS_LOCK:\n",
        "        job = JOBS.get(job_id)\n",
        "\n",
        "    if not job:\n",
        "        return jsonify({\"error\": \"job_not_found\"}), 404\n",
        "\n",
        "    if job.get(\"status\") != \"done\" or not job.get(\"output_path\"):\n",
        "        return jsonify({\"error\": \"not_ready\"}), 404\n",
        "\n",
        "    p = Path(job[\"output_path\"])\n",
        "    if not p.exists():\n",
        "        return jsonify({\"error\": \"file_missing\"}), 404\n",
        "\n",
        "    return send_file(str(p), mimetype=\"video/mp4\", as_attachment=False)\n"
      ],
      "metadata": {
        "id": "p8JE286lDqtP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PORT = 5000\n",
        "\n",
        "print(f\"Starting Flask server on port {PORT} ...\")\n",
        "app.run(host=\"0.0.0.0\", port=PORT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpcZcwr1DtWu",
        "outputId": "aa78d408-ae0b-4174-d290-726557e75f94"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Flask server on port 5000 ...\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared\n"
      ],
      "metadata": {
        "id": "gyS2cZoBEGmj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, re, time\n",
        "\n",
        "def start_cloudflared(port=5000):\n",
        "    cmd = [\n",
        "        \"./cloudflared\",         # ← FIXED\n",
        "        \"tunnel\",\n",
        "        \"--url\", f\"http://localhost:{port}\",\n",
        "        \"--no-autoupdate\"\n",
        "    ]\n",
        "\n",
        "    print(f\"[CF] Starting Cloudflared on port {port}…\")\n",
        "\n",
        "    proc = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True\n",
        "    )\n",
        "\n",
        "    url = None\n",
        "    while True:\n",
        "        line = proc.stdout.readline()\n",
        "        if not line:\n",
        "            break\n",
        "        print(\"[CF]\", line.strip())\n",
        "\n",
        "        if \"trycloudflare.com\" in line:\n",
        "            m = re.search(r\"(https://[a-zA-Z0-9\\-]+\\.trycloudflare\\.com)\", line)\n",
        "            if m:\n",
        "                url = m.group(1)\n",
        "                print(\"\\n[CF] Public URL:\", url, \"\\n\")\n",
        "                return url\n"
      ],
      "metadata": {
        "id": "JGT-HBOnG8p8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cf_url = start_cloudflared(5000)\n",
        "cf_url\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "Je3AbQBHHCnJ",
        "outputId": "0b554622-4ba5-4a53-f4bb-6e5c40e42ada"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CF] Starting Cloudflared on port 5000…\n",
            "[CF] 2025-12-08T09:16:12Z INF Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "[CF] 2025-12-08T09:16:12Z INF Requesting new quick Tunnel on trycloudflare.com...\n",
            "[CF] 2025-12-08T09:16:16Z INF +--------------------------------------------------------------------------------------------+\n",
            "[CF] 2025-12-08T09:16:16Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "[CF] 2025-12-08T09:16:16Z INF |  https://mighty-motherboard-studios-magnetic.trycloudflare.com                             |\n",
            "\n",
            "[CF] Public URL: https://mighty-motherboard-studios-magnetic.trycloudflare.com \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://mighty-motherboard-studios-magnetic.trycloudflare.com'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxDw57clHEwa",
        "outputId": "a1c4b969-8da6-4ee3-9602-dd58fca24f53"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K\n",
            "changed 22 packages in 3s\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def start_localtunnel(port=5000, subdomain=None):\n",
        "    cmd = [\"lt\", \"--port\", str(port)]\n",
        "    if subdomain:\n",
        "        cmd += [\"--subdomain\", subdomain]\n",
        "\n",
        "    print(f\"[LT] Starting LocalTunnel on port {port}…\")\n",
        "\n",
        "    proc = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True\n",
        "    )\n",
        "\n",
        "    url = None\n",
        "    while True:\n",
        "        line = proc.stdout.readline()\n",
        "        if not line:\n",
        "            break\n",
        "        print(\"[LT]\", line.strip())\n",
        "\n",
        "        if \"https://\" in line and \".loca.lt\" in line:\n",
        "            m = re.search(r\"(https://[a-zA-Z0-9\\-]+\\.loca\\.lt)\", line)\n",
        "            if m:\n",
        "                url = m.group(1)\n",
        "                print(\"\\n[LT] Public URL:\", url, \"\\n\")\n",
        "                return url\n"
      ],
      "metadata": {
        "id": "QAdtI0wwHQ70"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lt_url = start_localtunnel(5000)\n",
        "lt_url\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "inmwHxqVHU22",
        "outputId": "7e830271-f9fc-45b1-f701-09e1ae44ceb6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LT] Starting LocalTunnel on port 5000…\n",
            "[LT] your url is: https://bitter-states-leave.loca.lt\n",
            "\n",
            "[LT] Public URL: https://bitter-states-leave.loca.lt \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://bitter-states-leave.loca.lt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}