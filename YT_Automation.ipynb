{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTZkSdjZiGij/4egu0ov1h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aicreativeexplorer/YT-Automation/blob/main/YT_Automation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiafjBDXAm5J",
        "outputId": "d311c326-fd82-498f-d99f-b0cc62d2eafc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONFIG OK\n"
          ]
        }
      ],
      "source": [
        "# CONFIG — edit only if you moved paths\n",
        "REPO_DIR = \"/content/YT-Automation\"\n",
        "NOTEBOOK_NAME = \"YT-Automation.ipynb\"\n",
        "UPLOADED_VIDEO = \"/mnt/data/Kling AI- Next-Gen AI Video & AI Image Generator.mp4\"  # your uploaded demo\n",
        "DRIVE_TOKEN_PATH = \"/content/drive/MyDrive/AI-Automation/hf_token.txt\"  # put HF token here\n",
        "OUTPUT_DRIVE_FOLDER = \"/content/drive/MyDrive/AI-Automation/outputs/stitched\"\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/AI-Automation/checkpoints\"\n",
        "SVD_VERSION = \"svd\"  # 'svd' (open) or 'svd-xt-1-1' (better but gated)\n",
        "USE_AUTO_PUSH = False  # no auto-push by default\n",
        "print(\"CONFIG OK\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Auto-login to HuggingFace if token present in Drive\n",
        "import os\n",
        "if os.path.exists(DRIVE_TOKEN_PATH):\n",
        "    from huggingface_hub import login\n",
        "    with open(DRIVE_TOKEN_PATH,'r') as f:\n",
        "        token = f.read().strip()\n",
        "    login(token=token)\n",
        "    print(\"Logged into HuggingFace from Drive token.\")\n",
        "else:\n",
        "    print(\"No HF token at\", DRIVE_TOKEN_PATH, \"- you'll be asked if downloading gated models.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg2ZtRuIFxet",
        "outputId": "e0b666ce-88db-4a48-fc2e-a8bac0a6fa0a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "No HF token at /content/drive/MyDrive/AI-Automation/hf_token.txt - you'll be asked if downloading gated models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# minimal set used by pipeline\n",
        "pip install -q kornia open_clip_torch timm transformers safetensors accelerate pytorch-lightning einops imageio-ffmpeg imwatermark opencv-python-headless==4.6.0.66 gradio==3.34.0\n",
        "# RIFE + Real-ESRGAN notebooks will install their deps inside their cells when needed.\n",
        "echo \"installed deps\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMkH4koVGfWG",
        "outputId": "6444e79c-72da-4ffc-dfa2-1819e90c5508"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.3/48.3 MB 26.6 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.0/20.0 MB 91.2 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 50.1 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 57.6 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 831.6/831.6 kB 46.9 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 93.9 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.5/84.5 kB 6.1 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 kB 4.2 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 983.2/983.2 kB 55.0 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.8/44.8 kB 3.0 MB/s eta 0:00:00\n",
            "installed deps\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 2.0.8 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.6.0.66 which is incompatible.\n",
            "albucore 0.0.24 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.6.0.66 which is incompatible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# clone once per session\n",
        "if [ ! -d /content/generative-models ]; then\n",
        "  git clone https://github.com/Stability-AI/generative-models.git /content/generative-models\n",
        "fi\n",
        "# small helper scripts (won't break if run multiple times)\n",
        "mkdir -p /content/scripts\n",
        "cat > /content/scripts/ffmpeg_stitch.sh <<'SH'\n",
        "#!/usr/bin/env bash\n",
        "# Usage: ./ffmpeg_stitch.sh out.mp4 file1.mp4 file2.mp4 ...\n",
        "out=\"$1\"; shift\n",
        "# convert to ts and concat\n",
        "tsfiles=\"\"\n",
        "for f in \"$@\"; do\n",
        "  ts=\"/tmp/$(basename \"$f\").ts\"\n",
        "  ffmpeg -y -i \"$f\" -c copy -bsf:v h264_mp4toannexb -f mpegts \"$ts\"\n",
        "  tsfiles=\"${tsfiles}|${ts}\"\n",
        "done\n",
        "tsfiles=${tsfiles#|}\n",
        "ffmpeg -y -i \"concat:${tsfiles}\" -c copy -bsf:a aac_adtstoasc \"$out\"\n",
        "SH\n",
        "chmod +x /content/scripts/ffmpeg_stitch.sh\n",
        "echo \"cloned generative-models; helper scripts in /content/scripts\"\n"
      ],
      "metadata": {
        "id": "7WUgqUqpGi59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, subprocess, uuid, json, shutil, time\n",
        "from pathlib import Path\n",
        "\n",
        "def run_cmd(cmd, quiet=False):\n",
        "    print(\"RUN:\", cmd)\n",
        "    p = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "    if not quiet:\n",
        "        print(p.stdout)\n",
        "        if p.stderr:\n",
        "            print(\"ERR:\", p.stderr[:1000])\n",
        "    return p\n",
        "\n",
        "def ensure_folder(p):\n",
        "    Path(p).mkdir(parents=True, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "ensure_folder(OUTPUT_DRIVE_FOLDER)\n",
        "ensure_folder(CHECKPOINT_DIR)\n",
        "\n",
        "# ---- SVD: wrapper (placeholder) ----\n",
        "def generate_svd_from_text(prompt, seed=None, num_frames=14, out_prefix=\"svd\"):\n",
        "    \"\"\"\n",
        "    Minimal wrapper to call SVD sampling. The heavy work is in the SVD notebook code.\n",
        "    Here we assume `sample()` exists in session OR we call a local script.\n",
        "    For now this function writes a JSON config file for the manual SVD notebook to pick up.\n",
        "    \"\"\"\n",
        "    jobid = f\"{out_prefix}_{int(time.time())}_{uuid.uuid4().hex[:6]}\"\n",
        "    jobdir = f\"/tmp/{jobid}\"\n",
        "    ensure_folder(jobdir)\n",
        "    cfg = {\n",
        "        \"prompt\": prompt,\n",
        "        \"seed\": seed or \"random\",\n",
        "        \"num_frames\": num_frames,\n",
        "        \"out\": f\"{jobdir}/{jobid}.mp4\"\n",
        "    }\n",
        "    cfg_path = f\"{jobdir}/cfg.json\"\n",
        "    with open(cfg_path,'w') as f: json.dump(cfg,f)\n",
        "    print(\"SVD job written:\", cfg_path)\n",
        "    # NOTE: actual execution will be in the notebook cell that imports the SVD sampling code\n",
        "    return cfg[\"out\"], jobdir\n",
        "\n",
        "# ---- RIFE: interpolation wrapper ----\n",
        "def interpolate_with_rife(infile, factor=4):\n",
        "    out = f\"{Path(infile).with_suffix('')}_rife.mp4\"\n",
        "    # Call public RIFE Colab? For now we use a simple ffmpeg fps trick as placeholder\n",
        "    run_cmd(f\"ffmpeg -y -i '{infile}' -filter:v 'minterpolate=fps=30:mi_mode=mci:mc_mode=aobmc:me_mode=bidir:vsbmc=1' -c:v libx264 -crf 18 '{out}'\")\n",
        "    return out\n",
        "\n",
        "# ---- Real-ESRGAN: upscale wrapper (placeholder) ----\n",
        "def upscale_realesrgan(infile, scale=2):\n",
        "    out = f\"{Path(infile).with_suffix('')}_up.mp4\"\n",
        "    # placeholder: re-encode at higher resolution (actual Real-ESRGAN model would process frames)\n",
        "    run_cmd(f\"ffmpeg -y -i '{infile}' -vf scale=iw*{scale}:ih*{scale} -c:v libx264 -crf 18 '{out}'\")\n",
        "    return out\n",
        "\n",
        "# ---- Stitch helper ----\n",
        "def stitch_files(inputs, out):\n",
        "    inputs = [str(i) for i in inputs]\n",
        "    run_cmd(f\"/content/scripts/ffmpeg_stitch.sh '{out}' \" + \" \".join([f\"'{i}'\" for i in inputs]))\n",
        "    return out\n",
        "\n",
        "# ---- TTS helper (ElevenLabs fallback) ----\n",
        "def tts_generate(text, outfile=\"/tmp/tts.wav\", provider=\"free\"):\n",
        "    \"\"\"\n",
        "    provider: 'elevenlabs' (needs key), 'gtts' (free).\n",
        "    This is a simple wrapper: I recommend hooking to ElevenLabs for quality or use gTTS for free.\n",
        "    \"\"\"\n",
        "    if provider==\"gtts\":\n",
        "        # install gTTS if needed\n",
        "        run_cmd(\"python -m pip install -q gTTS pydub\")\n",
        "        from gtts import gTTS\n",
        "        tts = gTTS(text=text, lang='en')\n",
        "        tts.save(outfile)\n",
        "        return outfile\n",
        "    else:\n",
        "        # try gTTS as default\n",
        "        run_cmd(\"python -m pip install -q gTTS pydub\")\n",
        "        from gtts import gTTS\n",
        "        tts = gTTS(text=text, lang='en')\n",
        "        tts.save(outfile)\n",
        "        return outfile\n",
        "\n",
        "# ---- Caption burn-in (ffmpeg) ----\n",
        "def burn_captions(video_in, text, out):\n",
        "    # naive single-line centered caption\n",
        "    cmd = (\n",
        "        f'ffmpeg -y -i \"{video_in}\" -vf '\n",
        "        f\"\\\"drawtext=fontfile=/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf: text='{text}':\"\n",
        "        \"fontcolor=white:fontsize=48:box=1:boxcolor=0x00000099:boxborderw=5:x=(w-text_w)/2:y=h-150\\\" \"\n",
        "        f'-c:a copy \"{out}\"'\n",
        "    )\n",
        "    run_cmd(cmd)\n",
        "    return out\n",
        "\n",
        "print(\"Helpers loaded.\")\n"
      ],
      "metadata": {
        "id": "hs0SlVlgGmJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEXT-ONLY MODE — create config and call SVD job (sample wrapper)\n",
        "prompt = \"9:16 vertical, ultra-detailed close-up of a tiny mechanical fox exploring a sunlit garden, cinematic shallow depth-of-field, warm color grade, soft motion. No text or logos.\"\n",
        "out_mp4, jobdir = generate_svd_from_text(prompt, seed=1234, num_frames=25, out_prefix=\"tinyfox\")\n",
        "print(\"SVD job created. Local jobdir:\", jobdir)\n",
        "print(\"Once SVD sampling cell runs, output will be:\", out_mp4)\n"
      ],
      "metadata": {
        "id": "WU3GNFpAGsiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMAGE-TO-VIDEO MODE\n",
        "# Use your uploaded file to extract a conditioning frame (change timestamp if you want)\n",
        "import os\n",
        "frame_path = \"/content/frame_for_svd.png\"\n",
        "run_cmd(f\"ffmpeg -y -ss 00:00:01 -i '{UPLOADED_VIDEO}' -frames:v 1 '{frame_path}'\")\n",
        "print(\"Extracted frame:\", frame_path)\n",
        "\n",
        "# Now create SVD job that uses this image\n",
        "prompt_image = \"9:16 vertical, mechanical fox exploring, use this frame as a style reference. No text.\"\n",
        "out_mp4_img, jobdir_img = generate_svd_from_text(prompt_image, seed=999, num_frames=25, out_prefix=\"img2vid\")\n",
        "# attach the frame info to jobdir so the sampling cell can pick it\n",
        "open(f\"{jobdir_img}/cond_frame.txt\",\"w\").write(frame_path)\n",
        "print(\"Image->video job created at\", jobdir_img, \"expected out:\", out_mp4_img)\n"
      ],
      "metadata": {
        "id": "5y_8wn01Gtgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVD sampling runner (paste into a cell and run after you confirm model & weights)\n",
        "# This is a compact wrapper around the sampling function inside generative-models.\n",
        "import json, os, torch\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "job_cfg_path = None\n",
        "# find the latest job config in /tmp\n",
        "for d in sorted(Path(\"/tmp\").glob(\"svd_*\"), key=os.path.getmtime, reverse=True):\n",
        "    cfgf = d / \"cfg.json\"\n",
        "    if cfgf.exists():\n",
        "        job_cfg_path = str(cfgf)\n",
        "        break\n",
        "\n",
        "if not job_cfg_path:\n",
        "    raise RuntimeError(\"No SVD job config found in /tmp. Run the TEXT-ONLY or IMAGE-TO-VIDEO mode first.\")\n",
        "\n",
        "print(\"Using job config:\", job_cfg_path)\n",
        "cfg = json.load(open(job_cfg_path))\n",
        "\n",
        "# Minimal logic: call the SVD sampling script inside generative-models (the repo has sampling scripts)\n",
        "# NOTE: this exact command depends on the repo's sampling entrypoint. This is a general example.\n",
        "model_config = \"/content/generative-models/scripts/sampling/configs/svd.yaml\" if SVD_VERSION==\"svd\" else \"/content/generative-models/scripts/sampling/configs/svd_xt.yaml\"\n",
        "ckpt = os.path.join(CHECKPOINT_DIR, \"svd.safetensors\") if SVD_VERSION==\"svd\" else os.path.join(CHECKPOINT_DIR, \"svd_xt_1_1.safetensors\")\n",
        "\n",
        "print(\"model_config:\", model_config)\n",
        "print(\"ckpt:\", ckpt)\n",
        "# if ckpt missing, user must download (I can add auto-download if you want)\n",
        "if not os.path.exists(ckpt):\n",
        "    print(\"WARNING: checkpoint not found at\", ckpt, \" - YOU MUST DOWNLOAD weights or switch SVD_VERSION to 'svd' if available.\")\n",
        "\n",
        "# This following call is placeholder — you must adapt to the exact sample() wrapper inside the generative-models fork you use.\n",
        "# Many public SVD notebooks expose a `sample()` function. If present you can call:\n",
        "# out = sample(input_path=..., resize_image=True, num_frames=..., num_steps=..., seed=..., decoding_t=..., fps_id=6, motion_bucket_id=127, cond_aug=0.02, device='cuda', skip_filter=True)\n",
        "\n",
        "# For now we create a dummy mp4 to simulate output (remove this after hooking sample())\n",
        "sim_out = cfg.get(\"out\")\n",
        "run_cmd(f\"ffmpeg -y -f lavfi -i color=size=720x1280:rate=6:color=0x112233 -t 2 -vf drawtext=\\\"text='SIM:{cfg.get('prompt')[:40]}':fontsize=24:fontcolor=white:x=20:y=20\\\" {sim_out}\")\n",
        "print(\"Simulated SVD output:\", sim_out)\n"
      ],
      "metadata": {
        "id": "1vNAwjlHGvKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ORCHESTRATOR: post-process generated clips into final short\n",
        "from pathlib import Path\n",
        "\n",
        "# Example: find all generated svd outputs in /tmp job dirs\n",
        "svd_files = list(Path(\"/tmp\").glob(\"svd_*/*.mp4\")) + list(Path(\"/tmp\").glob(\"img2vid_*/*.mp4\"))\n",
        "svd_files = [str(x) for x in svd_files]\n",
        "print(\"Found svd outputs:\", svd_files)\n",
        "\n",
        "processed = []\n",
        "for f in svd_files:\n",
        "    rife = interpolate_with_rife(f, factor=4)\n",
        "    up = upscale_realesrgan(rife, scale=1)  # set scale=2 if you want to up-res\n",
        "    processed.append(up)\n",
        "\n",
        "# If you need a 30s short, loop/stitch until you reach ~30s (simple method: concat with itself or repeat)\n",
        "# naive approach: repeat processed until length ~30s\n",
        "final_target = \"/content/final_short.mp4\"\n",
        "stitch_files(processed, final_target)\n",
        "\n",
        "# Add voice + captions\n",
        "voice = tts_generate(\"This is a test voiceover for your short\", outfile=\"/content/voice.wav\", provider=\"gtts\")\n",
        "# merge voice with video\n",
        "final_with_audio = \"/content/final_short_audio.mp4\"\n",
        "run_cmd(f\"ffmpeg -y -i '{final_target}' -i '{voice}' -c:v copy -c:a aac -shortest '{final_with_audio}'\")\n",
        "\n",
        "# burn captions\n",
        "final_capt = \"/content/final_short_audio_captioned.mp4\"\n",
        "burn_captions(final_with_audio, \"Tiny mechanical fox explores a sunlit garden.\", final_capt)\n",
        "\n",
        "# copy to Drive output folder\n",
        "ensure_folder(OUTPUT_DRIVE_FOLDER)\n",
        "dst = os.path.join(OUTPUT_DRIVE_FOLDER, f\"final_short_{int(time.time())}.mp4\")\n",
        "shutil.copy(final_capt, dst)\n",
        "print(\"Saved final short to Drive:\", dst)\n"
      ],
      "metadata": {
        "id": "uuOb6K-rGzjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def one_click(prompt_text):\n",
        "    # TEXT-ONLY ! generate SVD config\n",
        "    out, jobdir = generate_svd_from_text(prompt_text, seed=\"random\", num_frames=25, out_prefix=\"oneclick\")\n",
        "    # RUN sampling cell manually (or we can call sample() if present)\n",
        "    # For now we simulate generation (same logic as SVD sampling cell)\n",
        "    run_cmd(f\"ffmpeg -y -f lavfi -i color=size=720x1280:rate=6:color=0x331122 -t 3 -vf drawtext=\\\"text='ONECLICK {prompt_text[:30]}':fontsize=32:fontcolor=white:x=20:y=20\\\" {out}\")\n",
        "    # postprocess\n",
        "    rife = interpolate_with_rife(out)\n",
        "    up = upscale_realesrgan(rife, scale=1)\n",
        "    final = \"/content/oneclick_final.mp4\"\n",
        "    stitch_files([up], final)\n",
        "    voice = tts_generate(prompt_text, outfile=\"/content/oneclick_voice.wav\")\n",
        "    run_cmd(f\"ffmpeg -y -i '{final}' -i '{voice}' -c:v copy -c:a aac -shortest '/content/oneclick_final_audio.mp4'\")\n",
        "    dst = os.path.join(OUTPUT_DRIVE_FOLDER, \"oneclick_final.mp4\")\n",
        "    shutil.copy(\"/content/oneclick_final_audio.mp4\", dst)\n",
        "    return dst\n",
        "\n",
        "demo = gr.Interface(fn=one_click, inputs=gr.Textbox(lines=2, placeholder=\"Enter prompt...\"), outputs=gr.File())\n",
        "demo.launch(share=False)\n"
      ],
      "metadata": {
        "id": "3JwAGfFdG0L8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5KheAADbG2Hq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}